<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Tobby Lie | RNN Music Generation</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body class="is-preload">

	<!-- Header -->
	<!-- <header id="header">
				<a href="index.html" class="title">Tobby Lie</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="elements.html">Elements</a></li>
					</ul>
				</nav>
			</header> -->

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<section id="main" class="wrapper">
			<div class="inner">
				<h1 class="major">Recurrent Neural Network Music Generation</h1>
				<span class="image fit"><img src="images/lstm_fig.png" alt="" /></span>
				<p>Historically music has always been an integral component of human life and interaction. Music has
					always been something innate to people and has served a powerful role in conveying powerful abstract
					emotions that humans often times have a difficult time dissecting. At the same time, peeling back
					the layers of a musical piece reveals that it is afterall just a sequence of notes strung together
					with some constraints such as key and time measure. The problem statement and motivation provides
					insight into why this topic is being explored and what fruit it actually can bear. We then move on
					to the methods we implemented in order to solve our problem. What we hope to achieve by the end of
					this paper is, given sequences of musical pieces in musical instrument digital interface (MIDI)
					format, is it possible for a long short-term memory (LSTM) to be trained over several epochs to
					learn to generate a unique sequence of notes, we explore an LSTM implementation for both
					ABC-notation and midi formats for input into our network and discover interesting strengths of both
					methods. We utilize an LSTM because it must have the ability to learn past of its ability to
					remember from past time steps in the network. After describing our methods we move on to discussing
					our results both in the actual turnout of our audio generation as well as plots describing loss and
					accuracy of our model during training. Finally, we discuss related works and limitations in our last
					portion, which pertains to implementations and ideas within a similar scope that have been done as
					well as limitations to the general goal of utilizing LSTMs to generate sequences of notes.</p>
				<ul class="actions">
					<li><a href="https://github.com/tobby-lie/Multi-Instrument-RNN-Generation/blob/master/Report.pdf"
							class="button primary fit">Read More about the Project</a></li>
				</ul>
			</div>
		</section>

	</div>


	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>