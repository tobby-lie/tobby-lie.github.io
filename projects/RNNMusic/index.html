<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
	<title>Tobby Lie | RNN Music Generation</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />

	<!--[if IE]><link rel="shortcut icon" href="favicon.ico"><![endif]-->
	<link rel="shortcut icon" href="images/TL-icon.png" type="image/x-icon">
	<link rel="icon" href="images/TL-icon.png" type="image/x-icon">

	<link rel="stylesheet" href="assets/css/main.css" />
	<noscript>
		<link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
</head>

<body class="is-preload">

	<!-- Wrapper -->
	<div id="wrapper">

		<!-- Main -->
		<section id="main" class="wrapper">
			<div class="inner">
				<h1 class="major">Recurrent Neural Network Music Generation</h1>
				<span class="image fit"><img src="images/lstm_fig.png" alt="" /></span>
				<p>Historically music has always been an integral component of human life and interaction. Music has
					always been something innate to people and has served a powerful role in conveying powerful
					abstract
					emotions that humans often times have a difficult time dissecting. At the same time, peeling
					back
					the layers of a musical piece reveals that it is afterall just a sequence of notes strung
					together
					with some constraints such as key and time measure. The problem statement and motivation
					provides
					insight into why this topic is being explored and what fruit it actually can bear. We then move
					on
					to the methods we implemented in order to solve our problem. What we hope to achieve by the end
					of
					this paper is, given sequences of musical pieces in musical instrument digital interface (MIDI)
					format, is it possible for a long short-term memory (LSTM) to be trained over several epochs to
					learn to generate a unique sequence of notes, we explore an LSTM implementation for both
					ABC-notation and midi formats for input into our network and discover interesting strengths of
					both
					methods. We utilize an LSTM because it must have the ability to learn past of its ability to
					remember from past time steps in the network. After describing our methods we move on to
					discussing
					our results both in the actual turnout of our audio generation as well as plots describing loss
					and
					accuracy of our model during training. Finally, we discuss related works and limitations in our
					last
					portion, which pertains to implementations and ideas within a similar scope that have been done
					as
					well as limitations to the general goal of utilizing LSTMs to generate sequences of notes.</p>
				<ul class="actions">
					<li><a href="RNN-Music_Report.pdf" class="button primary fit">Read More about the Project</a></li>
				</ul>
			</div>
		</section>

	</div>


	<!-- Scripts -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/jquery.scrollex.min.js"></script>
	<script src="assets/js/jquery.scrolly.min.js"></script>
	<script src="assets/js/browser.min.js"></script>
	<script src="assets/js/breakpoints.min.js"></script>
	<script src="assets/js/util.js"></script>
	<script src="assets/js/main.js"></script>

</body>

</html>